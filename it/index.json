[{"categories":null,"content":"Premessa ciao Codice Vedi la repository su Github Progetto Database ","date":"2023-09-27","objectID":"/it/database-project/:0:0","tags":["Project","MySql"],"title":"Progetto DataBase","uri":"/it/database-project/"},{"categories":null,"content":"Idea L‚Äôidea di questo progetto √® venuta ad un mio compagno di corso, Simone Zheng. L‚Äôidea √® quella di creare uno scraper che vada ad analizzare e ‚Äústudiare‚Äù i dati presenti sui siti di immobili, in modo da riuscire ad analizzare i vari annunci, selezionati per zona, e a calcolare quale tra le case/appartmenti in vendita conviene comprare e quali invece conviene non guardare proprio Attualmente, tutte le funzioni che si interessano di calcolare quale tra gli annunci sia il migliore, basandosi su stipendio medio, IRPEF e tc , sono ancora in fase di sviluppo Attualmente, lo scraper si occupa solo di prelevare i dati dai siti di immobili, e calcolare il costo medio per zona degli immobili. Codice Il codice dello scraper √® fatto in python. Le librerie utilizzate sono state : BeautifulSoup Pandas Requests Sys Getopt Sono state implementate anche le funzioni per prendere in input, da terminale, dei valori che andranno a settare la zona di interesse e il tipo di immobile che si vuol cercare Questo √® stato realizzato in questo modo arg_zone = \"\" arg_type = \"\" arg_help = \"Help: -z \u003czone\u003e -t \u003ctype\u003e\".format(argv[0]) try: opts, args = getopt.getopt(argv[1:], \"hz:t:\", [\"help\", \"zone=\", \"type=\"]) except: print(arg_help) sys.exit(2) for opt, arg in opts: if opt in (\"-h\", \"--help\"): print(arg_help) # print the help message sys.exit(2) elif opt in (\"-z\", \"--zone\"): arg_zone = arg elif opt in (\"-t\", \"--type\"): arg_type = arg if arg_type == \"all\": arg_type = \"case\" All‚Äôinterno troviamo anche una funzione che va a cercare quanti risulsati sono stati trovati sul sito, in modo da poter scorrere le varie pagine tramite un ciclo for, che incrementa il valore page= del sito. Il codice finale risulta essere questo # Importare moduli import requests import pandas as pd from bs4 import BeautifulSoup import sys import getopt import math # Indirizzo sito web def FindNumber(string): empty_string = \"\" for m in string: if m.isdigit(): empty_string = empty_string+m return int(empty_string) def SearchNumberOfElements(type,zone): url = f\"https://www.immobiliare.it/vendita-{type}/roma/{zone}/?criterio=rilevanza\u0026noAste=1\" response = requests.get(url) # Analizzare documento HTML del codice sorgente con BeautifulSoup html = BeautifulSoup(response.text, 'html.parser') number_of_el = html.find('div', class_=\"in-searchList__title\") value = number_of_el.text split = value.split() number = split[0] return int(number) def search(argv): # Import argument from terminal arg_zone = \"\" arg_type = \"\" arg_help = \"Help: -z \u003czone\u003e -t \u003ctype\u003e\".format(argv[0]) try: opts, args = getopt.getopt(argv[1:], \"hz:t:\", [\"help\", \"zone=\", \"type=\"]) except: print(arg_help) sys.exit(2) for opt, arg in opts: if opt in (\"-h\", \"--help\"): print(arg_help) # print the help message sys.exit(2) elif opt in (\"-z\", \"--zone\"): arg_zone = arg elif opt in (\"-t\", \"--type\"): arg_type = arg if arg_type == \"all\": arg_type = \"case\" locali_tot = [] price_tot = [] cycle = SearchNumberOfElements(arg_type,arg_zone) print(f\"Numeri di elementi trovati : {cycle}\") pagine = math.ceil(cycle/25) print(\"Inizio scansione e raccolta dati\") if pagine == 1: url = f\"https://www.immobiliare.it/vendita-{arg_type}/roma/{arg_zone}/?criterio=rilevanza\u0026noAste=1\" print(url) # Eseguire richiesta GET response = requests.get(url) # Analizzare documento HTML del codice sorgente con BeautifulSoup html = BeautifulSoup(response.text, 'html.parser') # Estrarre tutte le citazioni e gli autori dal documento HTML locali_html = html.find_all('a', class_=\"in-card__title\") price_html = html.find_all( 'li', class_=\"nd-list__item in-feat__item in-feat__item--main in-realEstateListCard__features--main\") # Raccogliere le citazioni in un elenco locali = list() for locale in locali_html: locali.append(locale.text) # Raccogliere gli autori in un elenco prices = list() for price in price_html: prices.append(price.text) locali_tot += locali price_tot += prices else: for i in range(1, pagine+1): url = f\"https://www.immobiliare.it/vendita-{arg_type}/roma/{arg_zo","date":"2023-04-24","objectID":"/it/webscraper/:0:0","tags":["Project","WebScraper","Python","Csv"],"title":"WebScraper","uri":"/it/webscraper/"},{"categories":null,"content":"P. S Piccolo reminder: Si ricorda che lo scraping di dati online √® del tutto LEGALE, la cosa imporatate √® che bisogna rimanere entra un certo range di scansioni al giorni, ma per il resto √® tutto legale dato che dal momento che un certo dato viene pubblicato su un sito web, ogni persona pu√≤ accedervi senza alcun problema. Grazie üòÑ ","date":"2023-04-24","objectID":"/it/webscraper/:0:1","tags":["Project","WebScraper","Python","Csv"],"title":"WebScraper","uri":"/it/webscraper/"},{"categories":null,"content":"P. P. S Lo scraper non √® neanche alla sua versione beta, oserei dire che si torva nella versione gamma ancora üòÑ, pertanto ogni critica/modifica/consiglio √® ben accetta/o Video Ecco in video per vedere lo scraper in azione Eseguito su SO Pop!_Os : ","date":"2023-04-24","objectID":"/it/webscraper/:0:2","tags":["Project","WebScraper","Python","Csv"],"title":"WebScraper","uri":"/it/webscraper/"},{"categories":null,"content":" Premessa, questo √® il progetto di cui vado pi√π fiero üòÑ Idea asdasd Realizzazione Servizi Gameplay Codice ","date":"2023-03-29","objectID":"/it/flappybird/:0:0","tags":["Project","FlappyBird","C sharp","MySql"],"title":"FlappyBird","uri":"/it/flappybird/"},{"categories":null,"content":"L‚Äôuniversit√† asdasasd Le difficolt√† Pensieri e consigli ","date":"2023-03-20","objectID":"/it/universit%C3%A0/:0:0","tags":["Universit√†"],"title":"Universit√†","uri":"/it/universit%C3%A0/"},{"categories":null,"content":"Idea L‚Äôidea mi √® venuta totalmente random, mentre non facevo un bel niente üòÇ Ho visto qualche video su youtube e mi sono deciso di approcciarmi a questa ‚Äúchallenge‚Äù (mettiamola tra virgolette perch√® poi si vedr√† che non √® una vera e propria challenge) L‚Äôidea che sta alla base √® molto semplice, un‚Äôinterfaccia (chiamiamola HOME) dove ci sono i collegamenti alle altre interfacce : Prelievo, Deposito, Show Utenti e cos' via‚Ä¶ Servizi I servizi offerti da questa app sono : Prelievo dei soldi dal conto Deposito soldi sul conto Show degli utenti Ricerca conto tramite nome account Cambio numero account tramite nome utente Realizzazione L‚Äôapp √® stata realizzata tramite il linguaggio Java, e utilizza un database MySql per lo storage e modifica dei dati L‚Äôide utilizzato √® Ecplise Tutte le interfaccie sono state realizzate tramite Java AWT, quindi il codice risulter√† poco chiaro (allego immagini delle interfacce) Codice Il codice di questa applicazione lo trovate tutto sul mio GitHub Link qui ‚Äì\u003e Bank App Queste sono gli screenshots delle varie interfacce Home Page: Home page Cambio Dettagli Account: Cambia Dettagli Account Prelievo: Prelievo Registra Utente: Registra Utenti Show degli utenti: Ricerca Utenti Database Il database √® stato realizzato su DBMS MySql, ed √® organizzato in questo modo: Ci sono due tabelle, ‚ÄúUtenti‚Äù e ‚ÄúCredito‚Äù Nella tabella Utenti ci sono i seguenti campi: UID -\u003e User ID (chiave primaria) User -\u003e Nome utente Pass -\u003e Password NConto -\u003e Numero del conto corrente Nella tabella Credito ci sono i seguenti campi: CID -\u003e Conto ID (chiave primaria) UserID -\u003e User ID (chiave secondaria relativa alla chaive UID della tabella Utenti) Credito -\u003e Credito del conto Ogni volta che un‚Äôutente viene inserito nello schema, gli viene dato un numero di conto in modo random, con credito 0, e nel DB viene collegato alla tabella Credito Le tabelle sono collegate da una relazione 1:1 ","date":"2023-03-10","objectID":"/it/bankapp/:0:0","tags":["Project","Bank App","Java","MySql"],"title":"Bank App","uri":"/it/bankapp/"},{"categories":null,"content":"Me Ciao, sono Salvucci Franco. Sono uno studente della Facolt√† di Informatica dell‚ÄôUniversit√† di Tor Vergata Sono sempre stato appassionato del mondo della programmazione e dell‚Äôinformatica in generale (sia lato software che hardware, ma soprattutto software üòÑ) Adoro sperimentare nuove tecnologie (come questa qui, ovvero Hugo) e adoro cimentarmi sempre in nuove sfide di progettazione/programmazione/ecc‚Ä¶ Attualmente sono particolarmente interessato al mondo degli Algoritmi, infatti probabilmente focalizzer√≤ la mia tesi su questo (ma per ora tralasciamo questo dettaglio, c‚Äô√® ancora un bel p√≤ di tempo per pensare a quello‚Ä¶) P.S : Dai un‚Äôocchiata al post che ho fatto sull‚Äôuniversit√†, dove racconto la mia esperienza personale, i miei dubbi, le mie paure e i miei consigli per i ragazzi appena iscritti -\u003e Universit√† Non so cosa altro dire, quindi direi che va bene cos√¨ Skills Queste sono le mie competenze: Linguaggi che conosco: C# Python Java C ///JQuery (un p√≤) LaTeX SQL Markdown IDE che uso: Visual Studio Code Visual Studio 2019 Ecplise SQL developer (SI lo so che non √® un IDE, ma un‚Äôinterfaccia grafica per il Db Oracle, ma non mi importa üòÑ) Altro: NeoVim Git GitHub Sto anche imparando ad usare linux tramite il sistema operativo Pop!_os Foto mia Link qui I miei progetti Ho realizzato vari software, a tempo perso, che mi hanno permesso di approfondire la conoscenza di vari linguaggi (C# e SQL in particolare) Eccoli: FlappyBird C# Version sviluppato in C# e SQL Blood Bank Management System App sviluppato in C# e SQL Bank App sviluppato in Java CryptoTool sviluppato in C# PortScanner sviluppato in Java (funziona solo in locale, ovviamente) Ops‚Ä¶ S√¨, ho dimenticato PHP, ma ammettiamolo, a chi cazzo piace PHP, siamo realistici (Ma s√¨, conosco anche PHP) ","date":"2023-03-04","objectID":"/it/aboutme/:0:0","tags":["Me","Biografy"],"title":"Su di me","uri":"/it/aboutme/"}]